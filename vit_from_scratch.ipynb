{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V7qRrTC7wud"
      },
      "source": [
        "# Implementation of Vision Transformer for building segmentation from INRIA aerial image labeling dataset using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8YTa39a7wuh"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d sagar100rathod/inria-aerial-image-labeling-dataset"
      ],
      "metadata": {
        "id": "TmK95rBs8GhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir data\n",
        "! unzip inria-aerial-image-labeling-dataset.zip -d data"
      ],
      "metadata": {
        "id": "_nKNXoU389SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USY_SMEk7wuh"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFogJ5l57wui"
      },
      "outputs": [],
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Pj6Pi97wuj"
      },
      "source": [
        "### Set device and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjsVWMaS7wuj"
      },
      "outputs": [],
      "source": [
        "# Set device to run on GPU if available\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# Set hyperparameters\n",
        "patch_size = 16\n",
        "latent_size = 768 # Embedding dimension (or latent vector) (16x16x3) (patch size 16x16) x3 color channels\n",
        "n_channels = 3 # Number of channels for input images\n",
        "num_heads = 12 # Number of head\n",
        "num_encoders = 12 # Number of encoder layers\n",
        "dropout = 0.1\n",
        "size = 224  # Size of the input image\n",
        "num_labels = 1 # Number of output labels (Building/Not building)\n",
        "\n",
        "epochs = 40\n",
        "lr = 1e-3   # Learning rate\n",
        "weight_decay = 0.03    # Weight decay\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8-J4xD67wuj"
      },
      "source": [
        "### Preprocess the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtthpqB77wuj"
      },
      "source": [
        "#### For training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5hzt19j7wuj"
      },
      "outputs": [],
      "source": [
        "# Calculate mean and standard deviation of the training images across all channels (R, G, B) for normalizing the dataset\n",
        "def compute_mean_std(image_dir):\n",
        "    training_images = os.listdir(image_dir)\n",
        "\n",
        "    # Initialize mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    # Process each image\n",
        "    for training_image in tqdm(training_images, desc=\"Processing images\"):\n",
        "        img_path = os.path.join(image_dir, training_image)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Convert image to tensor\n",
        "        img_tensor = ToTensor()(image) # Convert image to (C, H, W) tensor\n",
        "        # Calculate number of pixels\n",
        "        num_pixels += img_tensor.size(1) * img_tensor.size(2)\n",
        "        # Sum the mean and squared mean of each channel\n",
        "        mean += img_tensor.sum(dim=[1, 2])\n",
        "        std += (img_tensor ** 2).sum(dim=[1, 2])\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    mean /= num_pixels\n",
        "    std = (std / num_pixels - mean ** 2).sqrt()\n",
        "\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "image_dir = \"data/AerialImageDataset/train/images\"\n",
        "mean, std = compute_mean_std(image_dir)\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard deviation: {std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxiFMSfG7wuk"
      },
      "outputs": [],
      "source": [
        "# Resize the input images to 224x224\n",
        "transform_training_data = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.4048401415348053, 0.427262544631958, 0.3927135467529297],\n",
        "              std=[0.20133039355278015, 0.1835126429796219, 0.17614711821079254])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qICq3yqq7wuk"
      },
      "outputs": [],
      "source": [
        "# Class for INRIA training dataset\n",
        "class INRIATrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L') # Convert mask to grayscale\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            # Resize the mask to (224, 224) and convert to tensor\n",
        "            mask = Resize((224, 224))(mask)  # Resize the mask\n",
        "            mask = F.to_tensor(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "image_dir = 'data/AerialImageDataset/train/images'\n",
        "mask_dir = 'data/AerialImageDataset/train/gt'\n",
        "\n",
        "train_data = INRIATrainDataset(image_dir, mask_dir, transform=transform_training_data)\n",
        "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaoj_WJO7wuk"
      },
      "source": [
        "### Visualize the training data (image and mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRen8mi67wuk"
      },
      "outputs": [],
      "source": [
        "# Unnormalize the dataset for visualization\n",
        "def unnormalized(tensor, mean, std):\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "# Visualize the some preprocessed data\n",
        "def visualize_dataset(dataset, num_samples=5):\n",
        "    mean = [0.4048401415348053, 0.427262544631958, 0.3927135467529297]\n",
        "    std = [0.20133039355278015, 0.1835126429796219, 0.17614711821079254]\n",
        "\n",
        "    fig, axis = plt.subplots(num_samples, 2, figsize=(10, num_samples * 5))\n",
        "    for i in range(num_samples):\n",
        "        image, mask = dataset[i]\n",
        "\n",
        "        # Unnormalize image for visualization\n",
        "        image = unnormalized(image.clone(), mean, std)  # Clone to avoid modify the preprocessed data\n",
        "        image_np = image.permute(1, 2, 0).cpu().numpy()  # Convert image tensor to (height, width, channel)\n",
        "        mask_np = mask.squeeze().cpu().numpy()    # Convert mask tensor to numpy\n",
        "\n",
        "        # Show the image and corresponding mask\n",
        "        axis[i, 0].imshow(image_np)\n",
        "        axis[i, 0].set_title(f\"Image {i+1}\")\n",
        "        axis[i, 0].axis('off')\n",
        "\n",
        "        axis[i, 1].imshow(mask_np, cmap='gray')\n",
        "        axis[i, 1].set_title(f\"Mask {i+1}\")\n",
        "        axis[i, 1].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_dataset(train_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8wZfxg97wuk"
      },
      "source": [
        "#### For test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLbKWcWm7wuk"
      },
      "outputs": [],
      "source": [
        "# Calculate mean and standard deviation of the test images across all channels (R, G, B) for normalizing the dataset\n",
        "def compute_mean_std(image_dir):\n",
        "    test_images = os.listdir(image_dir)\n",
        "\n",
        "    # Initialize mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    # Process each image\n",
        "    for training_image in tqdm(test_images, desc=\"Processing test images\"):\n",
        "        img_path = os.path.join(image_dir, training_image)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Convert image to tensor\n",
        "        img_tensor = ToTensor()(image) # Convert image to (C, H, W) tensor\n",
        "        # Calculate number of pixels\n",
        "        num_pixels += img_tensor.size(1) * img_tensor.size(2)\n",
        "        # Sum the mean and squared mean of each channel\n",
        "        mean += img_tensor.sum(dim=[1, 2])\n",
        "        std += (img_tensor ** 2).sum(dim=[1, 2])\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    mean /= num_pixels\n",
        "    std = (std / num_pixels - mean ** 2).sqrt()\n",
        "\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "image_dir = \"data/AerialImageDataset/test/images\"\n",
        "mean, std = compute_mean_std(image_dir)\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard deviation: {std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOrKvC-07wul"
      },
      "outputs": [],
      "source": [
        "# Define transformations (e.g., resize, normalize) for test set images\n",
        "test_transforms = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.4543627202510834, 0.47366490960121155, 0.4127490520477295],\n",
        "              std=[0.20975154638290405, 0.1924573928117752, 0.18913407623767853])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SnXLS0Q7wul"
      },
      "outputs": [],
      "source": [
        "# Class for INRIA test dataset\n",
        "class INRIATestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "image_dir = 'data/AerialImageDataset/test/images'\n",
        "\n",
        "test_data = INRIATestDataset(image_dir, transform=transform_training_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWZbKSBY7wul"
      },
      "source": [
        "### Visualize test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZAJuBm57wul"
      },
      "outputs": [],
      "source": [
        "# Unnormalize the dataset for visualization\n",
        "def unnormalized(tensor, mean, std):\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "# Visualize the some preprocessed data\n",
        "def visualize_dataset(dataset, num_samples=5):\n",
        "    mean = [0.4543627202510834, 0.47366490960121155, 0.4127490520477295]\n",
        "    std = [0.20975154638290405, 0.1924573928117752, 0.18913407623767853]\n",
        "\n",
        "    fig, axis = plt.subplots(num_samples, 1, figsize=(10, num_samples * 5))\n",
        "    for i in range(num_samples):\n",
        "        image = dataset[i]\n",
        "\n",
        "        # Unnormalize image for visualization\n",
        "        image = unnormalized(image.clone(), mean, std)  # Clone to avoid modify the preprocessed data\n",
        "        image_np = image.permute(1, 2, 0).cpu().numpy()  # Convert image tensor to (height, width, channel)\n",
        "\n",
        "        # Show the image and corresponding mask\n",
        "        axis[i].imshow(image_np)\n",
        "        axis[i].set_title(f\"Test Image {i+1}\")\n",
        "        axis[i].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "visualize_dataset(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlRMSrBp7wul"
      },
      "source": [
        "## Building Vision Transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dettpawN7wul"
      },
      "source": [
        "### Input Embedding class. This class performs all the steps needed before the data goes into the ViT's encoder block. This includes splitting input images into patches, performing the linear projections of the patches to convert patches into vectors, adding a position embedding to the linear projection to provide spatial information about where each patch comes from the image. The output of this class will be fed into the Encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m2JCzG07wul"
      },
      "outputs": [],
      "source": [
        "# Create a subclass of Module (which is the base class for neural network modules)\n",
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size=patch_size, n_channels=n_channels, device=device, latent_size=latent_size, batch_size=batch_size):\n",
        "        super(InputEmbedding, self).__init__()\n",
        "        self.laten_size = latent_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
        "\n",
        "        self.linearProjection = nn.Linear(self.input_size, self.laten_size)\n",
        "\n",
        "        # Positional embedding\n",
        "        self.num_patches = (size // self.patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, self.laten_size)).to(self.device)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        input_data = input_data.to(self.device)\n",
        "        # Rearrange the image into patches\n",
        "        patches = einops.rearrange(input_data,\n",
        "                                   'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
        "        linear_projection = self.linearProjection(patches).to(self.device)\n",
        "        b, n, _ = linear_projection.shape       # Extract batch size, number of patches\n",
        "\n",
        "        # Add positional embedding to linear projection\n",
        "        linear_projection += self.pos_embedding\n",
        "        return linear_projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TStteJSo7wum"
      },
      "source": [
        "### Transformer Encoder class. The Transformer Encoder is composed of two main layers: Multi-Head Self-Attention and Multi-Layer Perceptron. Before passing patch embeddings through these two layers, we apply Layer Normalization and right after passing embeddings through both layers, we apply Residual Connection. There are 12 Transformer Encoders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzXEfFha7wum"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, latent_size=latent_size, num_heads=num_heads, device=device, dropout=dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.num_heads = num_heads\n",
        "        self.device = device\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(self.latent_size)\n",
        "        self.norm2 = nn.LayerNorm(self.latent_size)\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.multihead = nn.MultiheadAttention(self.latent_size, self.num_heads, dropout=self.dropout)\n",
        "\n",
        "        # MLP_head layer in the encoder. The ViT-Base variant uses MLP_head size 3072, which is latent_size*4\n",
        "        self.enc_MLP = nn.Sequential(\n",
        "            nn.Linear(self.latent_size, self.latent_size*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.latent_size*4, self.latent_size),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, embedded_patches):\n",
        "        # First sublayer: Norm + Multi-Head Attention + residual connection.\n",
        "        first_norm = self.norm1(embedded_patches)\n",
        "        attention_output, _ = self.multihead(first_norm, first_norm, first_norm)\n",
        "\n",
        "        # First residual connection\n",
        "        first_added_output = attention_output + embedded_patches\n",
        "\n",
        "        # Second sublayer: Norm + enc_MLP (Feed forward)\n",
        "        second_norm = self.norm2(first_added_output)\n",
        "        ff_output = self.enc_MLP(second_norm)\n",
        "\n",
        "        # Return the output of the second residual connection\n",
        "        return ff_output + first_added_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUhXylfo7wum"
      },
      "source": [
        "### Put together the whole Vision Transformer. What's added to the input embedding layer and the encoder stack here is the output MLP head, which is used for segmentation at the end of the whole model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvAppC87wum"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, device=device, num_labels=num_labels, dropout=dropout):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_encoders = num_encoders\n",
        "        self.latent_size = latent_size\n",
        "        self.device = device\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = InputEmbedding()\n",
        "\n",
        "        # Create a stack of encoder layers\n",
        "        self.enc_stack = nn.ModuleList([TransformerEncoder() for _ in range(self.num_encoders)])\n",
        "\n",
        "        # Segmentation head - reshape and output segmentation map\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            nn.Conv2d(latent_size, latent_size // 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(latent_size // 2, num_labels, kernel_size=1),  # n_classes for segmentation output\n",
        "            nn.Upsample(scale_factor=16, mode='bilinear', align_corners=False)  # Upsampling back to original resolution\n",
        "        )\n",
        "\n",
        "    def forward(self, test_input):\n",
        "        # Apply input embedding (patchify + linear projection + postional embedding)\n",
        "        enc_output = self.embedding(test_input)\n",
        "\n",
        "        # Loop through all encoder layers\n",
        "        for enc_layer in self.enc_stack:\n",
        "            enc_output = enc_layer(enc_output)\n",
        "\n",
        "        # Reshape and permute to match image dimensions\n",
        "        batch_size, num_patches, latent_size = enc_output.shape\n",
        "        H = W = size // patch_size\n",
        "        enc_output = enc_output.permute(0, 2, 1).contiguous().view(batch_size, latent_size, H, W)\n",
        "\n",
        "        # Apply segmentation head\n",
        "        seg_output = self.segmentation_head(enc_output)\n",
        "\n",
        "        return seg_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He5tjQIQ7wum"
      },
      "source": [
        "IoU metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bndrXJye7wum"
      },
      "outputs": [],
      "source": [
        "def iou_score(preds, targets, threshold=0.5):\n",
        "    preds = torch.sigmoid(preds)  # Apply sigmoid but don't threshold yet\n",
        "    preds = (preds > threshold).float()  # Threshold after sigmoid\n",
        "    preds_flat = preds.view(-1)\n",
        "    targets_flat = targets.view(-1)\n",
        "    intersection = (preds_flat * targets_flat).sum()\n",
        "    union = preds_flat.sum() + targets_flat.sum() - intersection\n",
        "    return (intersection + 1e-6) / (union + 1e-6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Furxm0tw7wum"
      },
      "source": [
        "### Call model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUex9U7-7wun"
      },
      "outputs": [],
      "source": [
        "model = VisionTransformer().to(device)\n",
        "# Use BCEWithLogitsLoss for segmentation\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "iou_metric = iou_score\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9554JbV7wun"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Store IoU scores for each epoch\n",
        "iou_scores = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_iou = 0.0\n",
        "    for inputs, masks in trainloader:\n",
        "        inputs, masks = inputs.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute running loss and IoU\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Move IoU to CPU and convert to scalar\n",
        "        running_iou += iou_metric(outputs, masks).cpu().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    epoch_iou = running_iou / len(trainloader)\n",
        "    iou_scores.append(epoch_iou)  # Save IoU for visualization\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, IoU: {epoch_iou:.4f}\")\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step(epoch_loss)\n",
        "\n",
        "# Save model after training\n",
        "torch.save(model.state_dict(), 'vit_segmentation_model.pth')\n",
        "\n",
        "# Visualize IoU score over epochs\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, epochs+1), iou_scores, marker='o', label='IoU')\n",
        "plt.title('IoU Score Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('IoU Score')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict on test set images"
      ],
      "metadata": {
        "id": "7s7IUKYZSTMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eft3AQgN7wun"
      },
      "outputs": [],
      "source": [
        "# Function to make predictions on the test set\n",
        "def predict_and_visualize(model, testloader, num_samples=5):\n",
        "    mean = [0.4543627202510834, 0.47366490960121155, 0.4127490520477295]\n",
        "    std = [0.20975154638290405, 0.1924573928117752, 0.18913407623767853]\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    fig, axis = plt.subplots(num_samples, 2, figsize=(10, num_samples * 5))\n",
        "    with torch.no_grad():\n",
        "        for i, image in enumerate(testloader):\n",
        "            if i == num_samples:\n",
        "                break\n",
        "\n",
        "            image = image.to(device)\n",
        "            pred_mask = model(image)  # Make prediction\n",
        "            pred_mask = torch.sigmoid(pred_mask)  # Apply sigmoid to get values in [0, 1]\n",
        "            pred_mask = pred_mask.squeeze().cpu().numpy()  # Convert to numpy array\n",
        "\n",
        "            # Unnormalize the image for display\n",
        "            image = unnormalized(image.clone(), mean, std)\n",
        "            image_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "            # Display the image and predicted mask\n",
        "            axis[i, 0].imshow(image_np)\n",
        "            axis[i, 0].set_title(f\"Test Image {i+1}\")\n",
        "            axis[i, 0].axis('off')\n",
        "\n",
        "            axis[i, 1].imshow(pred_mask, cmap='gray')\n",
        "            axis[i, 1].set_title(f\"Predicted Mask {i+1}\")\n",
        "            axis[i, 1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Load test data and visualize\n",
        "testloader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "predict_and_visualize(model, testloader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}